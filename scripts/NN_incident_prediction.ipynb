{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(nodes_in_hidden_layers, dropout, nr_of_inputs):\n",
    "    model = Sequential()\n",
    "    for i in range(len(nodes_in_hidden_layers)):\n",
    "        if i == 0:\n",
    "            model.add(Dense(nodes_in_hidden_layers[i], activation='swish', input_shape=(nr_of_inputs,)))  # Input and hidden layer\n",
    "        else:\n",
    "            if dropout != 0:\n",
    "                model.add(Dropout(dropout))\n",
    "            model.add(Dense(nodes_in_hidden_layers[i], activation='swish'))  # Additional hidden layers\n",
    "    model.add(Dense(1, activation=\"linear\"))  # Output layer\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"mean_squared_error\", metrics=(\"mean_squared_error\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_features(data):\n",
    "    station = np.eye(19)[data[\"Base Station\"]]\n",
    "    year = np.eye(4)[data[\"Year\"] - 2015]\n",
    "    month = np.eye(12)[data[\"Month\"] - 1]\n",
    "    day = np.eye(31)[data[\"Day\"] - 1]\n",
    "    week = np.eye(53)[data[\"Week\"] - 1]\n",
    "    weekday = np.eye(7)[data[\"Weekday\"] - 1]\n",
    "    hour = np.eye(24)[data[\"Hour\"]]\n",
    "\n",
    "    x = np.concatenate([\n",
    "        station,\n",
    "        year, \n",
    "        month, \n",
    "        day, \n",
    "        week, \n",
    "        weekday,\n",
    "        hour], axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(data):\n",
    "    station = np.eye(19)[data[\"Base Station\"]]\n",
    "    year = np.eye(4)[data[\"Year\"] - 2015]\n",
    "    month = np.eye(12)[data[\"Month\"] - 1]\n",
    "    day = np.eye(31)[data[\"Day\"] - 1]\n",
    "    week = np.eye(53)[data[\"Week\"] - 1]\n",
    "    weekday = np.eye(7)[data[\"Weekday\"] - 1]\n",
    "    hour = np.eye(24)[data[\"Hour\"]]\n",
    "    daytime = np.zeros((len(data), 6))\n",
    "\n",
    "    # Determine season based on month (assuming 4 seasons: 0-Winter, 1-Spring, 2-Summer, 3-Autumn)\n",
    "    season = np.zeros((len(data), 4))\n",
    "    season[:, 0] = np.logical_or(month[:, 0] == 11, month[:, 0] <= 2)  # Winter\n",
    "    season[:, 1] = np.logical_and(month[:, 0] >= 3, month[:, 0] <= 5)  # Spring\n",
    "    season[:, 2] = np.logical_and(month[:, 0] >= 6, month[:, 0] <= 8)  # Summer\n",
    "    season[:, 3] = np.logical_and(month[:, 0] >= 9, month[:, 0] <= 10)  # Autumn\n",
    "    \n",
    "    # Determine if it's a weekend (1-Weekend, 0-Not a weekend)\n",
    "    weekend = np.logical_or(weekday[:, 5] == 1, weekday[:, 6] == 1)\n",
    "    weekend = np.expand_dims(weekend.astype(int), axis=1)\n",
    "    \n",
    "    # Loop over the four-hour periods and sum the one hot encoded values of each hour in the period\n",
    "    for i in range(6):\n",
    "        daytime[:, i] = np.sum(hour[:, i*4:(i+1)*4], axis=1)\n",
    "\n",
    "    x = np.concatenate([\n",
    "        station,\n",
    "        year, \n",
    "        month, \n",
    "        day, \n",
    "        week, \n",
    "        weekday,\n",
    "        hour,\n",
    "        season,\n",
    "        weekend,\n",
    "        #daytime\n",
    "    ], axis=1)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_features(data):\n",
    "    def get_daytime(hour, period_hours):\n",
    "        periods = int(24/period_hours)\n",
    "        for i in range(periods):\n",
    "            start_hour = i\n",
    "            end_hour = (i+1) * period_hours\n",
    "            if start_hour <= hour < end_hour:\n",
    "                return i\n",
    "        return 6\n",
    "\n",
    "    # Map the Hour column to the Daytime column using the map_hour_to_daytime function\n",
    "    data[\"Daytime\"] = data[\"Hour\"].apply(lambda x: get_daytime(x, 4))\n",
    "\n",
    "    return data[[\"Base Station\", \"Year\", \"Month\", \"Day\", \"Week\", \"Weekday\", \"Hour\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert_base_features(data):\n",
    "    base_station = np.argmax(data[:, :19], axis=1) \n",
    "    year = np.argmax(data[:, 19:23], axis=1) + 2015\n",
    "    month = np.argmax(data[:, 23:35], axis=1) + 1\n",
    "    day = np.argmax(data[:, 35:66], axis=1) + 1\n",
    "    week = np.argmax(data[:, 66:119], axis=1) + 1\n",
    "    weekday = np.argmax(data[:, 119:126], axis=1) + 1\n",
    "    hour = np.argmax(data[:, 126:], axis=1)\n",
    "\n",
    "    return list(zip(base_station, year, month, day, week, weekday, hour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert_features(data):\n",
    "    base_station = np.argmax(data[:, :19], axis=1) \n",
    "    year = np.argmax(data[:, 19:23], axis=1) + 2015\n",
    "    month = np.argmax(data[:, 23:35], axis=1) + 1\n",
    "    day = np.argmax(data[:, 35:66], axis=1) + 1\n",
    "    week = np.argmax(data[:, 66:119], axis=1) + 1\n",
    "    weekday = np.argmax(data[:, 119:126], axis=1) + 1\n",
    "    hour = np.argmax(data[:, 126:150], axis=1)\n",
    "    season = np.argmax(data[:, 150:154], axis=1)\n",
    "    weekend = np.argmax(data[:, 154:155], axis=1)\n",
    "\n",
    "    return list(zip(base_station, year, month, day, week, weekday, hour, season, weekend))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_date(data, start_date, end_date):\n",
    "    datetime_columns = ['Year', 'Month', 'Day', 'Hour']\n",
    "    datetime_data = pd.to_datetime(data[datetime_columns])\n",
    "    \n",
    "    mask = (datetime_data.dt.floor('h').between(start_date, end_date, inclusive=True))\n",
    "\n",
    "    data_train = data[~mask]    \n",
    "    data_test = data[mask]\n",
    "\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, s):\n",
    "    data_station = data\n",
    "    if s != None:\n",
    "        data_station = data[data[\"Base Station\"] == s]\n",
    "    x = get_features(data_station)\n",
    "    y = data_station[\"Incidents\"].to_numpy()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTRIBUTION_FILE = \"data/incident_distribution/station.csv\"\n",
    "\n",
    "data = pd.read_csv(DISTRIBUTION_FILE, encoding='utf-8', escapechar='\\\\', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime(2017, 8, 7, 0, 0, 0)\n",
    "end_date = datetime(2017, 8, 13, 23, 59, 59)\n",
    "data_train, data_test = train_test_split_date(data, start_date, end_date)\n",
    "\n",
    "\"\"\" start_date = datetime(2017, 8, 6)\n",
    "end_date = datetime(2017, 8, 22)\n",
    "_, data_validation = train_test_split_date(data, start_date, end_date)\n",
    "X_validation, Y_validation = preprocess(data_test, None) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime(2018, 8, 1, 0, 0, 0)\n",
    "end_date = datetime(2018, 8, 31, 0, 0, 0)\n",
    "data_train, data_val = train_test_split_date(data_train, start_date, end_date)\n",
    "\n",
    "X_train, Y_train = preprocess(data_train, None)\n",
    "X_test, Y_test = preprocess(data_test, None)\n",
    "X_val, Y_val = preprocess(data_val, None)\n",
    "\n",
    "#X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "layers = [64, 32]\n",
    "model = get_model(layers, 0.4, len(X_train[0]))\n",
    "es = EarlyStopping(monitor='val_mean_squared_error', mode='min', verbose=1, patience=10)\n",
    "model.fit(X_train, Y_train, epochs=30, validation_data=(X_val, Y_val), verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime(2018, 8, 1, 0, 0, 0)\n",
    "end_date = datetime(2018, 8, 31, 0, 0, 0)\n",
    "data_train, data_val = train_test_split_date(data_train, start_date, end_date)\n",
    "\n",
    "X_train, Y_train = preprocess(data_train, None)\n",
    "X_test, Y_test = preprocess(data_test, None)\n",
    "X_val, Y_val = preprocess(data_val, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = []\n",
    "layerss = [[64]]\n",
    "for layers in layerss:\n",
    "    for _ in range(5):\n",
    "        model = get_model(layers, 0.4, len(X_train[0]))\n",
    "        es = EarlyStopping(monitor='val_mean_squared_error', mode='min', verbose=1, patience=5)\n",
    "        history = model.fit(X_train, Y_train, epochs=30, validation_data=(X_val, Y_val), verbose=1, callbacks=[es])\n",
    "        ce.append(min(history.history['val_mean_squared_error']))\n",
    "    avg_ce = sum(ce) / len(ce)\n",
    "    print(layers, avg_ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_model = sm.GLM(Y_train, X_train, family=sm.families.Poisson())\n",
    "poisson_results = poisson_model.fit()\n",
    "val_pred_poisson = poisson_results.predict(X_val)\n",
    "mse_poisson = mean_squared_error(Y_val, val_pred_poisson)\n",
    "print(\"Mean squared error (Poisson):\", mse_poisson)\n",
    "\n",
    "nb_model = sm.GLM(Y_train, X_train, family=sm.families.NegativeBinomial())\n",
    "nb_results = nb_model.fit()\n",
    "val_pred_nb = nb_results.predict(X_val)\n",
    "mse_nb = mean_squared_error(Y_val, val_pred_nb)\n",
    "print(\"Mean squared error (NB):\", mse_nb)\n",
    "\n",
    "\"\"\" zinb_model = sm.ZeroInflatedNegativeBinomialP(endog=Y_train, exog=X_train, exog_infl=np.log(X_train + 1e-8))\n",
    "zinb_results = zinb_model.fit()\n",
    "val_pred_zinb = zinb_results.predict(X_val, exog_infl=np.log(X_val + 1e-8))\n",
    "mse_zinb = mean_squared_error(Y_val, val_pred_zinb)\n",
    "print(\"Mean squared error (ZINB):\", mse_zinb)\n",
    "\n",
    "zip_model = sm.ZeroInflatedPoisson(endog=Y_train, exog=X_train, exog_infl=np.log(X_train + 1e-8))\n",
    "zip_results = zip_model.fit()\n",
    "val_pred_zip = zip_results.predict(X_val, exog_infl=np.log(X_val + 1e-8))\n",
    "mse_zip = mean_squared_error(Y_val, val_pred_zip)\n",
    "print(\"Mean squared error (ZIP):\", mse_zip) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(x_train, y_train):\n",
    "    kf = KFold(n_splits=5)\n",
    "    ce = []\n",
    "    for train_index, validation_index in kf.split(x_train):\n",
    "        model = get_model([128, 32, 8], 0.4, len(x_train[0]))\n",
    "        es = EarlyStopping(monitor='val_mean_squared_error', mode='min', verbose=1, patience=5)\n",
    "        history = model.fit(x_train[train_index], y_train[train_index], validation_data=(x_train[validation_index], y_train[validation_index]), epochs=30, verbose=1, callbacks=[es])\n",
    "        ce.append(min(history.history['val_mean_squared_error']))\n",
    "    avg_ce = sum(ce) / len(ce)\n",
    "    return avg_ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = preprocess(data_train, None)\n",
    "X_test, Y_test = preprocess(data_test, None)\n",
    "\n",
    "avg = cross_validation(X_train, Y_train)\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "Y_test = []\n",
    "X_test = []\n",
    "\n",
    "for s in range(0, 19):\n",
    "    X_train, Y_train = preprocess(data_train, s)\n",
    "    X_test_s, Y_test_s = preprocess(data_test, s)\n",
    "\n",
    "    layers = [128, 64, 16]\n",
    "    model = get_model(layers, 0.4, len(X_train[0]))\n",
    "    es = EarlyStopping(monitor='mean_squared_error', mode='min', verbose=1, patience=1)\n",
    "    model.fit(X_train, Y_train, epochs=20, verbose=1, callbacks=[es])\n",
    "\n",
    "    \"\"\"# Fit the Poisson regression model\n",
    "    model = sm.GLM(Y_train, X_train, family=sm.families.Poisson())\n",
    "    model = sm.GLM(Y_train, X_train, family=sm.families.NegativeBinomial())\n",
    "    model = sm.ZeroInflatedGeneralizedPoisson(Y_train, X_train, exog_infl=X_train, inflation='logit')\n",
    "    model = model.fit() \"\"\"\n",
    "\n",
    "    predictions_station = model.predict(X_test_s, verbose=0)\n",
    "    print(mean_squared_error(Y_test_s, predictions_station))\n",
    "\n",
    "    predictions.extend(predictions_station)\n",
    "    X_test.extend(X_test_s)\n",
    "    Y_test.extend(Y_test_s)\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "Y_test = np.array(Y_test)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Poisson regression model\n",
    "model = sm.GLM(Y_train, X_train, family=sm.families.Poisson())\n",
    "model = model.fit()\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(Y_test, predictions)\n",
    "print(\"Mean squared error: \", mse)\n",
    "\n",
    "X_test_reverted = revert_features(X_test)\n",
    "\n",
    "print(\"Base Station\", \"Year\", \"Month\", \"Day\", \"Week\", \"Weekday\", \"Hour\", \"Season\", \"Weekend\", \"Daytime\")\n",
    "for i in range(len(predictions)):\n",
    "    print(predictions[i], np.round(predictions[i], 2), Y_test[i], X_test_reverted[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS_FILE = \"data/incident_distribution/station_predictions_3.json\"\n",
    "\n",
    "def save_predictions(predictions, X_test_reverted):\n",
    "    d = {}\n",
    "    for station in range(0, 19):\n",
    "        d[station] = {}\n",
    "        for day in range(6, 14):\n",
    "            d[station][day] = {}\n",
    "            for hour in range(0, 24):\n",
    "                d[station][day][hour] = {}\n",
    "    for prediction, x in zip(predictions, X_test_reverted):\n",
    "\n",
    "        station = x[0]\n",
    "        day = x[3]\n",
    "        hour = x[6]\n",
    "        if day == 14:\n",
    "            continue\n",
    "    \n",
    "        d[station][day][hour] = round(float(prediction), 3)\n",
    "    print(\"Saving predictions to file...\")\n",
    "    with open(PREDICTIONS_FILE, 'w') as f:\n",
    "        json.dump(d, f, indent=2)\n",
    "\n",
    "\n",
    "save_predictions(predictions, X_test_reverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS_FILE = \"data/incident_distribution/station_predictions_3.json\"\n",
    "\n",
    "def save_predictions(predictions, X_test_reverted):\n",
    "    d = {}\n",
    "    with open(PREDICTIONS_FILE, 'r') as f:\n",
    "        d = json.load(f)\n",
    "    for station in range(0, 19):\n",
    "        for day in range(14, 22):\n",
    "            d[str(station)][str(day)] = {}\n",
    "            for hour in range(0, 24):\n",
    "                d[str(station)][str(day)][str(hour)] = {}\n",
    "    for prediction, x in zip(predictions, X_test_reverted):\n",
    "\n",
    "        station = x[0]\n",
    "        day = x[3]\n",
    "        hour = x[6]\n",
    "    \n",
    "        d[str(station)][str(day)][str(hour)] = round(float(prediction), 3)\n",
    "    print(\"Saving predictions to file...\")\n",
    "    with open(PREDICTIONS_FILE, 'w') as f:\n",
    "        json.dump(d, f, indent=2)\n",
    "\n",
    "\n",
    "save_predictions(predictions, X_test_reverted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
